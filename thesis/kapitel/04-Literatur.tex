\chapter{Stand der Technik} \label{literatur}

Folgend werden bestehende literarische Grundlagen für die vorliegende Arbeit aus Forschung und Entwicklung kurz zusammengefasst und die wichtigsten Punkte aufgezeigt. Der Inhalt befasst sich vorwiegend mit Konzepten, Theorien und Begrifflichkeiten, welche für das tiefere Verständnis der vorliegenden Thematik unabdingbar sind. Auch wurden bestehende Verfahren, welche ähnliche Ziele haben, genauer untersucht.


%\url{http://www2.hawaii.edu/~lipyeow/ics624/2011spr/extracting-concepts-1.pdf}\\
%\url{https://www.dropbox.com/s/9un841s3xw8k3m9/Parameswaran%202010%20-%20Towards%20The%20Web%20Of%20Concepts%20Extracting%20Concepts%20from%20Large%20Datasets.pdf?dl=0}\\
%\url{https://www.burakkanber.com/blog/machine-learning-full-text-search-in-javascript-relevnce-scoring/}\\

\section{\gls{Artifical Intelligence}}
\gls{Artifical Intelligence} bezeichnet im Allgemeinen den Versuche eines Systems menschliches Verhalten beziehungsweise Intelligenz zu automatisieren oder zu simulieren. Die Definition solcher Systeme kann grundsätzlich in vier Kategorien aufgeteilt werden: 
\begin{itemize}
    \item System, die wie Menschen denken.\\
    \item System, die rational denken.\\
    \item System, die wie Menschen handeln.\\
    \item System, die rational denken.\\
\end{itemize}
Menschliche Systeme unterscheiden sich von rationalen, indem sie versuchen Entscheidung mit dem Menschen gleichen oder ähnlichen Prozessen zu fällen versuchen. Während rationale Systeme Entscheidungen basierend auf den vorliegenden Daten treffen. Diese Unterschiede zeigen gleichzeitig auch die Evolution in der Forschung im Bereich von \gls{Artifical Intelligence} an. Bis 1970 wurde versucht ein System zu entwickeln, welches wie ein Mensch denkt und handelt. Von 1970 an gelangten jedoch Systeme mit einer rationalen Intelligenz in den Fokus, welche jederzeit die richtige Entscheidung treffen können.\\
Systeme, welche Verfahren aus dem Feld von \gls{Artifical Intelligence} anwenden, werden auch Agenten genannt. Abhängig vom gewählten Verfahren, entscheiden solche Agenten, auf Grund unterschiedlicher Argumentation, wie auf ein Ereignis zu reagieren ist. Dabei werden drei unterschiedliche Ebenen der Argumentation unterschieden:
\begin{itemize}
    \item \textbf{Assoziative Argumentation}, was wenn der Agent bestimmte Ereignisse beobachtet?\\
    \item \textbf{Ursächliche Argumentation}, was wenn der Agent bestimmte Aktionen durchführt?\\
    \item \textbf{Gegensätzliche Argumentation}, was wenn der Agent bestimmte Aktionen nicht durchgeführt hätte?\\
\end{itemize}
Diese verschiedenen Ebenen unterstützen die Entscheidung zur Auswahl Teildaten, welche für den jeweiligen Anwedungsfall wichtig sind.
So beeinflussen sich die Anzahl von Verbindungen einer Mobilantenne neben einem Autobahnabschnitt und das Stau-Aufkommen auf dem gleichen Abschnitt gegenseitig. Während der Wochentag und der Geburtstags des Beifahrers keinen Einfluss auf das Stauaufkommen haben. Dies obwohl diese zu diesem Zeitpunkt in einer Beziehung zu einander stehen.\\
\cite{Russell:2009:AIM:1671238}
\section{\gls{Machine Learning}}
Neben \gls{Natural Language Processing} oder \gls{Computer Vision} ist \gls{Machine Learning} ein weiterer Aspekt aus dem Bereich von \gls{Artifical Intelligence}. Dabei gewinnt ein Agent, auf Grund von Mustern innerhalb von Daten neue Informationen. Dieser Vorgang wird allgemein als Lernen bezeichnet. Grundsätzlich können folgende drei Kategorien des Lernens unterschieden werden:
\begin{itemize}
    \item \textbf{Supervised learning}, anhand von beobachteten und beschrifteten Ereignisse und den dazugehörigen Aktionen lernt der Agent eine Funktion, mit welcher er auf zukünftige Ereignisse reagieren kann. Zum Beispiel die Erkennung von Früchten auf Bildern, anhand einer Bildmenge, welche mit der jeweiligen Frucht beschriftet ist.\\
    \item \textbf{Unsupervised learning}, innerhalb von beobachteten und unbeschrifteten Daten erkennt der Agent Muster, welche er nutzt, um auf zukünftige Ereignisse zu reagieren. So kann beispielweise ein Navigationssystem selbstständig Tage mit Stau aufgrund der zurückgelegten Strecke und der Fahrzeit erkennen.
    So erkennt beispielsweise ein Navigationssystem Tage mit viel Stau anhand der Fahrzeit für eine zurückgelegte Strecke. Dies funktioniert ohne Datensätze, welche einen Tag mit viel Stau beschreiben.\\
    \item \textbf{Reinforcement learning}, basierend auf einer Belohnung lernt ein Agent, mit welcher Reaktion er eine bestimmten Situation entgegnen muss. So kann zum Beispiel ein lauffähiger Roboter lernen, wie er Hindernisse im Gelände bewältigen kann. Dabei ist es denkbar, dass die Aktion durch die verschiedenen Winkel seiner Gelenke und der Geschwindigkeit gegeben ist. Diese optimiert er anschliessend anhand der Belohnung. Die Belohnung ist ein Indikator für Erfolg oder Nichterfolg.\\
\end{itemize}
\cite{Russell:2009:AIM:1671238}

\section{\gls{Natural Language Processing}}\label{natural-language-processing}
Unter \gls{Natural Language Processing} wird die maschinelle Verarbeitung von natürlicher Sprache (Wort und Schrift) verstanden. Dabei werden Methoden und Erkenntnisse aus der klassischen \gls{Linguistik} angewendet. So können Wortarten bestimmt, Wortstämme gebildet oder Satzstrukturen analysiert werden. Mit Hilfe solcher Informationen können Algorithmen formuliert werden, um zum Beispiel eine Stimmungsanalyse von Texten durchzuführen. 


The complexity barrier -> the statistical revolution

\begin{quote}
Generally speaking, systems based entirely on natural language concepts are not at all competitive with systems based on statistical analysis of texts.
\\\cite{kantor2001foundations}\end{quote}

Grundsätzlich sind Algorithmen, welche ausschliesslich auf Konzepten der natürlichen Sprache aufbauen, nicht vergleichbar mit denjenigen, welche lediglich auf statistischen Analysen beruhen. Aber sprachwissenschaftliche Konzepte als zusätzliche Basis einer Analyse sind durchaus wertvoll.


\subsection{Tokenization}\label{tokenization}

Ein wichtiger Begriff in diesem Kontext ist \textit{Tokenization}. Ein zu verarbeitender Text ist in diesem Stadium repräsentiert durch einen langen String, welcher aus einer aneinandergereihten Zeichne besteht. Der Vorgang des \textit{Tokenization} teilt diesen Text zunächst nach Satz- und Sonderzeichen auf. Anschliessend werden die Sätze und Satzteile (beispielsweise nach Leerschlägen) in einzelne Einheiten (Tokens) aufgesplittet. Diese können in einem späteren Schritt in bestimmte syntaktische Klassen eingeteilt werden.\\\cite{grefenstette1994word}

\subsection{Stemming}
Beim \textit{Stemming} geht es um die Entfernung von Suffixen eines Wortes. Wörter die grundsätzlich eine ähnliche Bedeutung oder einen ähnlichen Ursprung haben, besitzen oftmals einen gemeinsamen Wortstamm. Durch den Verzicht auf Suffix können gemeinsame Wort\-stäm\-me gefunden werden.
\\\cite{porter1980algorithm}

Dies kann beispielsweise soweit gehen, dass Nomen und Verben auf einen gemeinsamen Wortstamm reduziert werden können. Dies ist aber nicht das in jeder Situation gewünschte Endergebnis, denn oftmals ist es wichtig die Wortarten unterscheiden zu können. Mit \textit{Stemming} ist immer auch ein Informationsverlust verbunden. 


\subsection{Stopwords}

% https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf
Als \textit{Stopwords} bezeichnet man üblicherweise sehr häufig vorkommende Wörter, welche zwar eine grammatikalische Funktion haben aber für die Gewinnung von Informationen keine weitere Bedeutung haben. Deren Funktion ist eher von von syntaktischer anstelle semantischer Natur. Für die Ermittlung dieser \textit{Stopwords} gibt es sowohl statistische als auch heuristische (regelbasierte) Verfahren.
\\\cite{wilbur1992automatic,manning2008introduction}


\subsection{Part of Speech Tagging}\label{part-of-speech}

% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.2604&rep=rep1&type=pdf

\cite{manning1999foundations,brill1992simple,brill1994some}


% http://www.dtic.mil/dtic/tr/fulltext/u2/a460532.pdf

%\textbf{Allgemeine Einleitung:Stemming, Stopwords, Nomen, Verbe, Tokenization, part of speech tagger}


\url{https://files.ifi.uzh.ch/cl/siclemat/lehre/papers/GrefenstetteTapanainen1994.pdf}

\url{http://www.ijcscn.com/Documents/Volumes/vol5issue1/ijcscn2015050102.pdf}

%\url{http://download.springer.com/static/pdf/656/art%253A10.1023%252FA%253A1011424425034.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1023%2FA%3A1011424425034&token2=exp=1493200110~acl=%2Fstatic%2Fpdf%2F656%2Fart%25253A10.1023%25252FA%25253A1011424425034.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1023%252FA%253A1011424425034*~hmac=9ee34173989c9cb90c6095be2974563812abfbac8cea1624eba8fc12482eeb73&robot=noindex}

\section{\gls{Textanalyse}}
Durch den Prozess der \gls{Textanalyse} können Informationen aus einem Text extrahiert werden. Basierend auf diesen Informationen werden Anwendungen wie eine \gls{Volltextsuche} oder die Erkennung von Ähnlichkeiten oder wichtigen Stichworten ermöglicht. Je nach Anwendungsfall werden statistische Verfahren mit Techniken aus dem Feld von \gls{Natural Language Processing} (\autoref{natural-language-processing}) kombiniert. Ein Beispiel für \gls{Textanalyse} ist die \gls{Keyword Extraction} (\autoref{keyword-extraction}).
\\\cite{tan1999text} 

% Text Mining

\subsection{\gls{Volltextsuche}}
Unter \gls{Volltextsuche} versteht man die Funktionalität eine Menge von Wörtern innerhalb eines Dokumentenkorpus zu suchen und bei Erfolg auch zu finden und dann die entsprechenden Dokumente aufzuzeigen. Die Grundlage für die \gls{Volltextsuche} bildet der Volltextindex, welcher alle in einem Text vorhandenen Wörter beinhaltet.

\subsection{Schlüsselwörter}
 % http://link.springer.com/chapter/10.1007%2F11775300_8
\subsection{Keyphrase}

\subsection{\gls{Extraction}}\label{keyword-extraction}

\subsubsection{Keywords}

Ein mögliches Resultat der \gls{Textanalyse} stellen Schlüsselwörter (\gls{Keyword}[s]) dar. Sie fassen im Idealfall den zugrundeliegende Text kurz und prägnant zusammen, stellen damit wichtigste semantische Informationen dar. Weiter bilden sie eine wichtige Grundlage für Dokumentenanalyse und -klassifizierung. 
\\\cite{Zhang2006}[S. 85]


Für die Generierung dieser \gls{Keyword}[s] benutzt man Verfahren der \gls{Keyword Extraction}. Hierbei geht es um die Auswahl einer kleinen Menge an Wörter / Begriffe aus einem Text, welche den Inhalt oder die Bedeutung dieses möglichst gut widerspiegeln.
\\\cite{hulth2004}

\subsubsection{TF-IDF}

\subsubsection{Vektorspace}

\subsubsection{Cosine Distance}

\subsubsection{Okapi BM25}

\subsubsection{Stanfordpaper}

Diese \gls{Keyword}[s] bestehen aus einem oder mehreren \gls{Konzept}[en]. Konzepte beziehen sich auf Objekte, Entitäten, Ereignisse und Themen, welche für suchende Benutzer interessant sein könnten (\cite{dalvi2009web}). Die Definition ist bewusst offen gewählt, da sich eine genauere Eingrenzung schwierig gestaltet. Eine weiterführende Definition geht in Richtung Nutzen für Menschen: Sobald eine gewisse Anzahl von Personen etwas als Konzept erkennt, kann dieses als Konzept angesehen werden (\cite{parameswaran2010towards}). Das Nutzung von \gls{Konzept}[en] findet unter anderen bei Suchmaschinen, automatischem Tagging statt.

Nach der Auswahl aller potentiellen \gls{Keyword}[s] wird zunächst versucht semantisch nicht wertvolle Begriffe auszuschliessen. Dies ist ein wichtiger Schritt, denn die anschliessende Berechnung der Relevanz ist ein sehr aufwändiger Prozess. 

% https://people.dsv.su.se/~hulth/thesis_hulth.pdf
% ähnliche Arbeit? http://ceur-ws.org/Vol-706/poster13.pdf 


% Web of Concepts http://s3.amazonaws.com/academia.edu.documents/33513575/A_Web_of_Concepts.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1493217891&Signature=bpzInfxK7UR6S%2FFuOGsr4nJCdG8%3D&response-content-disposition=inline%3B%20filename%3DA_Web_of_Concepts.pdf

Vorgeschlagene Methoden für diesen Vorgang nehmen oftmals sogenannte globale Kontext-Informationen in Gebrauch. Diese beinhalten unter anderem die Häufigkeit eines Begriffes innerhalb eines Dokuments und auch die Häufigkeit eines Begriffes innerhalb des gesamten Dokumentenkorpus.
\\\cite{Zhang2006}[S. 85] % http://link.springer.com/chapter/10.1007%2F11775300_8
% This kind of global information ignores the term's local context information and makes the extraction performance limited.


\subsubsection{Häufigkeit und Gewichtung}\label{gewichtung}

Ein Dokument, welches einen gesuchten Begriff öfters enthält als andere, ist mit grosser Wahrscheinlichkeit von grösserer Bedeutung als eines, welches den Begriff weniger oft enthält. Die Häufigkeit eines gesuchten Begriffes innerhalb eines Dokumentes ist somit ein wichtiger Teil der Gewichtung. Die Gewichtung ist die Grundlage zum Vergleich der Relevanz der verschiedenen Begriffes. Der einfachste Ansatz der Bestimmung dieser Gewichtung besteht darin diese mit der Vorkommenshäufigkeit innerhalb eines Dokumentes gleichzusetzen. Diesen Gewichtung bezeichnet man als \textit{term frequency}. In der Notation als $\text{tf}_{t,d}$ zu finden, wobei $\text{t}$ für einen Begriff (term) innerhalb eines Dokumentes $\text{d}$ steht.

Die alleinige Beachtung der Häufikeit zur Bestimmung der Relevanz bringt aber Probleme, alle Begriffe werden als gleichermassen wichtig eingestuft. Bestimmte Begriff können aber von vornherein direkt als weniger wichtig eingestuft werden. Kommt ein Begriff in einem Korpus durchgängig in allen Dokumenten häufig vor, ist es für ein einzelnes Dokument keine geeignete Grundlage für die Bestimmung der Relevanz. Denn dieses Wort hebt einen Text gegenüber dem Korpus nicht ab, ist somit keine ausreichende Repräsentation des Inhaltes.

Ein erster Ansatz von Manning (\cite{manning2008introduction}) ist die Gewichtung auf Grund der Vorkommens\-häufigkeit eines Begriffes innerhalb des Korpus zu skalieren. Dieser Wert wird als \textit{collection frequency} (cf) bezeichnet. Je häuf\-iger der Begriff, umso tiefer die Gewichtung. Begriffe, welche im Allgemeinen häufig auftauchen würden so in ihrer Relevanz niedriger. 

\begin{longtable}{|p{2cm}| p{1cm} | p{1cm}|}
  \hline
    Wort & cf & df \\\hline
    try & 10422 & 8760 \\\hline
    insurance & 10440 & 3997 \\\hline
    \caption{Vergleich cf und df \protect\footnotemark
    }
  \label{tab:vergleich-cf-df}
\end{longtable}
\footnotetext{\cite{manning2008introduction}[S.118]}


Da aber versucht wird mit einer Gewichtung auf Basis von unterschiedlichen Dokumenten zu arbeiten, macht es Sinn ebenfalls einen Wert auf selber Basis zu verwenden. Darum wird die Anzahl Dokumente verwendet, welche einen bestimmten Begriff enthalten. Dieser Wert wird als Dokumentenhäufigkeit (\textit{document frequency}, df) bezeichnet und mit $\text{df}$ notiert.

Der Grund, warum die \textit{document frequency} bevorzugt wird, zeigt ist in \autoref{tab:vergleich-cf-df} aufgezeigt. Der cf-Wert der beiden Wörter 'insurance' und 'try' ist in etwa identisch. Doch beim df-Wert gibt es einen grossen Unterschied. Die beiden Werte können sich somit also durchaus sehr unterschiedlich verhalten. Zusätzlich kann man noch anmerken, dass der df-Wert mehr dem intitiv erwarteten Resultat entspricht. Ein Schlüsselwort 'insurance' sollte relevanter sein als ein Schlüsselwort 'try'.

Um nun die Gewichtung mit Basis der Dokumentenhäufigkeit zu skalieren, wird folgende Formel eingeführt:

\[
\text{idf}_{t}=\text{log}\frac{N}{\text{df}_{t}}
\]

Diese bezeichnet man als inverse Dokumentenhäufigkeit (\textit{inverse document frequency} (idf)) für einen Begriff $\text{t}$. Die Gesamtanzahl der Dokumente ist durch $N$ gegeben.
\\\cite{manning2008introduction}

\subsubsection{TF-IDF}

TF-IDF ist ein Gewichtungsalgorithmus zur Bestimmung der Relevanz eines Begriffes. Wie obig schon erwähnt, nimmt er Bezug auf globale Kontext-Informationen. Er kombiniert die auf den im \autoref{gewichtung} definierten Werte.

\[
    \text{tf-idf}_{\text{t,d}} = \text{tf}_{\text{t,d}}\times\text{idf}_\text{f}
\]

Der TF-IDF Wert ist hoch, wenn ein Begriff in einem Dokument oft, in allen anderen Dokumenten hingegen nicht vorkommt. Er ist tief, wenn der Begriff in vielen oder allen Dokumenten vorkommt.

% https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf

\textbf{Document Vector, Normalisierung, ..}

\cite{manning2008introduction}

%tf = term frequency, nr. of appearance of a term in a doc\\

%idf = inverse document frequency, as rare the as higher the value, log(number all docs/number of docs with term)\\

%tf-idf = tf x idf\\

\subsubsection{Heuristik}

\url{https://nlp.stanford.edu/IR-book/pdf/06vect.pdf}\\

\subsubsection{Okapi BM25}
based on tf-idf\\
tunable params k1 and b to modulate term frequency saturation and field length normalization\\
term frequency saturation, above a significant number of usage of a term, docs will get the same score neither the term appears 40 times or 100 times\\
field length normalization, consider length of doc againts length of all docs, combine results of title and description

%http://kak.tx0.org/IR/TFxIDF

\section{React}

\section{Architekturpatterns}

\begin{itemize}
    \item MicroServices
    \item Message oriented middleware
    \item Messaging
    \item Message Passing
\end{itemize}

\section{Kommunikation}\label{literatur-kommunikation} 
Websockets bieten eine asnychrone bidirektionale (full duplex) Kommunikation zwischen Client und Server über einen Kanal, welcher über einen einzigen Socket läuft. Dies bedeutet eine enorme Reduktion von unnötigem Netzwerkverkehr und Latenz im Vergleich (Long-)Polling-Lösungen. Dies vor allem, da versucht wird mittels zwei Verbindungen das Verhalten von Websockets zu simulieren. Websockets können automatisch mit netzwerkspezifischen Umständen, wie Proxy-Server oder Firewalls, umgehen und machen so eine Verbindung über jede Verbindung möglich.\\\cite{AboutHTM97:online}
% http://websocket.org/aboutwebsocket.html

Socket.io ist eine WebSocket API, welche den Umgang mit Websockets erleichtert. Beispielsweise erkennt socket.io selbstständig, ob Web\-So\-cket in der verwendeten Umgebung unterstützt werden und sucht gegebenenfalls Alternativen. \\\cite{WebSocke14:online, prusty2016modern}

% https://davidwalsh.name/websocket



\section{Datentransfer}

Für einen asynchronen Datentransfer von grossen Dateien über das Netzwerk sind \gls{Stream}[s] eine Möglichkeit. Sie funktionieren prinzipiell analog zu grundlegenden Streams in Unix-Systemen. Streams teilen die Gesamtheit der zu sendenden oder zu empfangenden Daten in eine Sequenz von kleineren Daten auf. Die Übertragung verläuft kontinuierlich ab.

% https://github.com/nkzawa/socket.io-stream
% http://msgpack.org
% https://nodejs.org/api/stream.html
% https://github.com/substack/stream-handbook
% https://books.google.ch/books?hl=de&lr=&id=YgdbZbkTDkoC&oi=fnd&pg=PT9&dq=socket.io&ots=TVDh5nhPCQ&sig=kcHauykErOKQuNAtqxfNs5JahxM#v=onepage&q=socket.io&f=false

%streams, pakete, Protokoll

% https://allesagil.net/category/projektmanagement/

\section{Persistenz}

\subsection{Dropbox}
begrenzt von API, grosse Datenmenge, viele Files

\subsection{stor.j}
beta

\subsection{\gls{SFTP}}
einfach, schnell, sicher, SSH
