\chapter{Lösungsdesign}

Das Lösungsdesign beinhaltet die Grundlagen für die erfolgreiche Umsetzung des Prototypen. Basierend auf den Anforderungen und der Aufgabenstellung wird die Software-Architektur definiert. Wäh\-rend dies auf einer hohen Abstraktionsebene geschieht, erfolgt in einem nächsten Schritt der Entwurf und das Design der Software. Dabei dient die Architektur als Leitplanke, welche zusammen mit der Recherche über den aktuellen Stand der Technik (\autoref{literatur}) zu einem konkreten Entwurf führen.

\section{Architektur}\label{architecture}

Der Architektur-Entwurf betrachtet die zu entwickelnde Software aus einer abstrakten Sicht, wobei die tatsächliche verwendete Software, keinen Einfluss hat. Das Ziel ist eine grundsätzliche Übersicht über die Software, deren Komponenten, Schnittstellen und auch deren Verteilung.

\input{kapitel/03-Architektur}
% ----------------------------------------------------------------


\section{Algorithmus}
Der Kern des Prototypen bilden die verschiedene Algorithmen. Basierend auf den externen Datenquellen ermöglichen sie eine Volltext-Suche und die Extraktion der relevanten \gls{Keyword}[s]. Nachfolgend werden die Funktionsweise als auch wichtigsten Überlegungen dazu veranschaulicht. Basierend auf verschiedenen bestehenden Ansätzen wird die Lösungsfindung für den hier verwendeten Algorithmus dargelegt. 


Um die relevanten \gls{Keyword}[s] für ein spezifisches Dokument zu extrahieren werden verschiedenen Methoden aus dem Feld von \hyperref[natural-language-processing]{\textit{Natural Language Processing}} kombiniert mit statistischen Analsysen und heuristischen Vorgaben. Für die Analyse der \gls{Keyword}[s] werden \gls{N-Gramm}[e] der grösse eins bis vier berücksichtigt. Nebst möglichst treffenden Resultaten liegt der Fokus auch auf der raschen Aussortierung von ungeeigneten Kandidaten. Da die Berchnung der Relvanz der Rechen- und Speicher-Intensivste Vorgang des Algorithmus ist.  Die Anzahl mögliche \gls{N-Gramm}[en] ist definiert als: 
\[f(x)=\sum_{n=0}^N x - n  
\begin{cases} 
   (x - n)  & \text{if } x > n \\
   0      & \text{if } x \leq n
  \end{cases}\]
Wobei $N$ die maximale Länge eines \gls{N-Gramm} und $x$ die Länge des Textes representieren.

In der \autoref{fig:seq_keywordextraction} wird dafür Konzeptionelle Ablauf graphisch dargestellt und anschliessend die verschiedenen Elemente genauer erläutert.

    \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{KeywordExtraction}
    \caption{Ablauf Schlüsselwort-Extraktion}
    \label{fig:seq_keywordextraction}
    \end{figure}
 

\subsection{Eingabe}
Als Eingabe in den Algorithmus wird eine Text erwartet. Im Fall dieses Prototypen handelt es sich ein Wikipedia Artikel aus den Trainings Daten. Um die verschiedenen folgenden Schritt besser zu illustrieren, wird der folgende Satz als Beispiel Text verwendete. Mit einer Länge von $19$ Wörter, besitzt er $70$ mögliche \gls{N-Gramm}[e] mit Länge eins bis vier.

\begin{quote}
\textit{The Guardian newspaper was founded in 1821 as \glqq The Manchester Guardian\grqq, which head office is still located in Manchester.}
\end{quote}
Mit einer Länge von $19$ Wörter, besitzt er $70$ mögliche \gls{N-Gramm}[e] mit Länge eins bis vier.

\subsection{(1) Vorverarbeitung}
Im ersten Schritt wird der Text anhand verschiedener Vorgaben vorbereitet. Dazu werden zu Beginn nach einem Punkt, einem Fragezeichen, Ausrufezeichen oder einem Zeilenschaltung, der nachfolgende Buchstaben in die Kleinschreibung umgewandelt. Dies dient dazu Wörter zu erkennen, welche als Nomen genutzt werden jedoch nicht aus diese Wortart stammen. Dadurch verändert sich der Beispiel Satz geringfügig. 

\begin{quote}
\textit{\textbf{t}he Guardian newspaper was founded in 1821 as \glqq The Manchester Guardian\grqq, which head office is still located in Manchester.}
\end{quote}

Weiter wird nun der Text in Fragmente aufgeteilt. Dabei wird davon ausgegangen, dass sich relevante \gls{Keyword}[s] nicht über eine Satzzeichen erstrecken. Es entsteht eine Abfolge von Text Fragmenten.
\begin{quote}
\textit{\textbf{(}the Guardian newspaper was founded in 1821 as\textbf{)} \textbf{(}The Manchester Guardian\textbf{)} \textbf{(}which head office is still located in Manchester\textbf{)}}
\end{quote}
Nach dem ersten Schritt wurde der Text mit $19+18+17+16=70$ möglichen \gls{N-Gramm}[en] in eine List von Text-Fragmente mit $(8+7+6+5)+(3+2+1)+(8+7+6+5)=58$ möglichen \gls{N-Gramm}[en] transformiert. Dadurch konnten bereis $12$ Kandidaten ausgeschlossen werden.


\subsection{(2) Text zerlegung}
Bevor die eigendliche Kandidaten gebildetet werden, werden nun die Text-Fragmente durch das \hyperref[tokenization]{\textit{Tokenization}} aufgeteilt in eine List von einzelnen Wörter. Ebenfalls werden dabei unerwünscht Sonderzeichen am Anfang oder am Ende eines Wortes entfernt. Dabei handelt es sich in erster Linie um alle anderen Satzzeichen welche bei der Aufteilung des Textes in Text-Fragmente nicht berücksichtigt wurden.

\begin{quote}
\textit{\textbf{[}the, Guardian, newspaper, was, founded, in, 1821, as\textbf{]}, \textbf{[}The, Manchester, Guardian\textbf{]}, \textbf{[}which, head, office, is, still, located, in, Manchester\textbf{]}}
\end{quote}

\subsection{(3) Generierung möglicher Keywords}
Anhand der generierten Wort Listen können nun die verschiedenen \gls{N-Gramm}[e] generiert werden. Es entsteht eine List von \gls{N-Gramm}[en] der Länge von eins bis vier.

\begin{quote}
\textit{\textbf{[}the Guardian newspaper was, the Guardian newspaper, the Guardian, the, ... , located in Manchester, located in, located, in Manchester, in, Manchester\textbf{]}}
\end{quote}

\subsection{(4) Filter mittels POS-Tagger}
Wie von \cite{parameswaran2010towards} beschrieben folgen Relevante \gls{Keyword}[s] bestimmten grammatikalischen Regeln. Diese sind wie folgt definiert:
\begin{enumerate}
    \item Ein relevantes \gls{Keyword} besteht mindestens aus einem Nomen. Somit werden entfallen \gls{Keyword}[s] wie zum Beispiel \textit{was} oder \textit{located in}.
    \item Ein relevantes \gls{Keyword} beginnt nicht mit einem Pronomen, Verb oder Partikel. Diese Regel ist in der Lage \gls{Keyword}[s] wie \textit{the Guardian newspaper} auszusortieren.
    \item Ein relevantes \gls{Keyword} endet nicht mit einem Pronomen, Verb oder Partikel. Diese Regel ist in der Lage \gls{Keyword}[s] wie \textit{Guardian newspaper was} auszusortieren.
\end{enumerate}
Wie bereits erwähnt möchten auch Nicht-Nomen welche als solche verwendetet werden berücksichtigt werden. Dafür werden die zwei Regeln mit einer dritten erweitert:
\begin{enumerate}
    \item[4.] Ein relevantes \gls{Keyword} beginnt gross. Wie bereits erwähnt wurden vor gängig innerhalb des Textes Wörter nach bestimmten Satzzeichen in die Kleinschreibung umgewandelt, damit diese in diesem Schritt nicht beachtet werden. Dadurch wird zum Beispiel \textit{The Manchester Guardian} weiter beachtet auch wenn er grundsätzlich der zweiten Regel widerspricht da er mit einem Pronomon startet. 
\end{enumerate}
Um diese Regeln zu Überprüfen wird ein \hyperref[part-of-speech]{Part-Of-Speech-Tagger} verwendet. Dabei handelt es sich um Algorithmus mit welchem die Worarten von gegebenen Wörter bestimmt werden kann. Um den Regeln zu entsprechen muss ein \gls{Keyword} sowohl Regeln 1\&3 folgend, zusätzlich entweder der Regel 2, Regel 4 oder beiden. Mittels dieser drei Regeln können alle möglichen Kandidaten ausser den folgenden ignoriert werden:

\begin{quote}
\textit{\textbf{[}Guardian newspaper, Guardian, newspaper, 1821, The Manchester Guardian, The Manchester, The, Manchester Guardian, Manchester, Guardian, head office, head, office, Manchester\textbf{]}}
\end{quote}

Damit sind noch $14$ mögliche \gls{Keyword}[s] von ursprünglich $70$. Anschliessend werden gleiche \gls{Keyword}[s] zusammengfasst indem sie zusammen mit ihrer Anzahl Vorkomnisse kombiniert werden, somit reduziert sich die Anzahl unterschiedlichen \gls{Keyword}[s] auf $12$ (\autoref{keyword-with-count}). Ebenfalls wird jeweils das erste Wort eines \gls{Keyword}[s] kleingeschrieben falls es sich nicht um ein Nomen handelt. So kann sichergestellt werden, dass zum Beispiel das 1-Gramm "The" im nächsten Schritt tiefer gewertet wird. Da die Grosschreibung nur im Kontext des \gls{Keyword}[s] \textit{The Manchester Guardian} von Bedeutung ist.

\begin{longtable}{|p{4cm}| p{1cm}|}
  \hline
    \gls{Keyword} & \#\\\hline
    Guardian & 2  \\\hline
    Manchester & 2  \\\hline
    Guardian newspaper & 1  \\\hline
    newspaper & 1  \\\hline
    1821 & 1  \\\hline
    the Manchester Guardian & 1  \\\hline
    the Manchester & 1  \\\hline
    the & 1  \\\hline
    Manchester Guardian & 1  \\\hline
    head office & 1  \\\hline
    head & 1  \\\hline
    office & 1  \\\hline
        \caption{Keyword mit Vorkomniss}
    \label{keyword-with-count}
\end{longtable}

\subsection{(5) Berechnung der Relevanz}\label{calcrelevance}
Um die verschiedenen \gls{Keyword}[s] zu gewichten wird ein individueller Score pro \gls{Keyword} ausgerechnent. Dazu werden folgende Metriken benötigt (\autoref{metric-per-keyword}).

\begin{longtable}{|p{2cm}| p{8cm}|}
  \hline
    Metrik & Erläuterung\\\hline
    $numDocs$ & Anzahl Dokument im Korpus  \\\hline
    $docFreq$ & Anzahl Dokument im Korpus welche diese \gls{Keyword}[s] enthalten  \\\hline
    $docLength$ & Anzahl Wörter des spezifischen Dokumentes \\\hline
    $freq$ & Haufigkeit des \gls{Keyword}[s] innerhalb des Dokuments. \\\hline
    \caption{Benötigte Metriken}
    \label{metric-per-keyword}
\end{longtable}


Basierend auf den eingeführten Metriken wird mit Hilfe einer angepassten Version der \textit{Similarity}-Formel\footnotemark \footnotetext{\cite{TFIDFSim65:online}} des \textit{Apache Lucene} Projekts den jeweiligen Score berechnet.

\begin{equation}\label{tf}
tf = \sqrt{freq} 
\end{equation}
\begin{equation}\label{idf}
idf  =  1 +log( \frac{numDocs}{1 + docFreq})
\end{equation}
\begin{equation}\label{lengthNorm}
lengthNorm =\frac{1}{\sqrt[3]{docLength}} 
\end{equation}
\begin{equation}\label{tfidf}
tfidf = tf * idf^2 * lengthNorm
\end{equation}

\autoref{tf} repräsentiert die Häufigkeit des \gls{Keyword}[s] innerhalb des gegebenen Dokument. Wenn die Anzahl steigt so steigt auch der \textit{tf}-Wert. Um die Empfindlichkeit zu reduzieren, wird der Wert gedämpft mittels der Wurzel-Funktion. 

Neben der Häufigkeit innerhalb des Dokuments ist die inverse Häu\-fig\-keit (\autoref{idf}) ein wichtiger Wert. Dadurch wird die Einzigartigkeit des \gls{Keyword}[s] innerhalb des Korpus ausgedrückt. Dieser Wert sinkt je öfter das gleiche \gls{Keyword} in andere Dokumente enthalten ist. Mittels der Addition von $1$ im Nenner innerhalb der Logarithmus Funktion wird eine Division durch Null verhindert. 

Würde mittels der \autoref{tf} \& \autoref{idf} der Score berechnet werden, so würden kurz und lange Dokumente Werte in andere Bereichen produzieren. Eine Verwendungen eines Schwellwerte für Begrenzung der \gls{Keyword}[s] wäre unmöglich. Dazu wird der Wert mit Hilfe der Dokumenten-Länge normalisiert. Dazu wird mittels der \autoref{lengthNorm} der Score normalisiert

\subsection{(6) Auswahl}
Schlussendlich werden die berechneten \gls{Keyword}[s] mithilfe eines Schw\-el\-werts begrenzt, somit wird eine Sinnvolle Anzahl an Dokumenten zurückgegeben. Im Gegensatz zu einer Limitierung der Anzahl \gls{Keyword}[s] kann mit einem Schwellwert je nach Text eine Anzahl an \gls{Keyword}[s] zurückgegeben werden, welche für den Text ideal ist.

\subsection{Dokumente für Schlüsselwort}
Die Gegenteilige Operation der \gls{Keyword} Extraktion ist Extraktion aller Dokument für ein bestimmtes  \gls{Keyword}. Dabei wird eine List an Dokumente erwartet, in welche ein bestimmtes \gls{Keyword} einen Höhen Score ausweisen kann. 

    \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{DocumentForKeyword}
    \caption{Ablauf Relevante Dokumente für Schlüsselwort}
    \label{fig:seqdocforkeyword}
    \end{figure}
\begin{enumerate}
    \item  Dazu werden in einem ersten Schritt alle möglichen Dokumente ausgewählt. In dieser Menge an Dokumenten kommt das entsprechenden \gls{Keyword} mindestens einmal vor. Jedoch ungeachtet der Relvanz des \gls{Keyword}[s] für das jeweilige Dokument.
    \item In einem nächsten Schritt wird für jedes Dokument den jeweiligen Score für das bestimmte \gls{Keyword} berechnent. Die Berechnung folgt dabei exakt der bereits gesehenen Berchnungen der Relevanz (\autoref{calcrelevance}).

    \item Nun werden ebenfalls die Anzahl an Resultate mit Hilfe eines Schwellwerts reduziert. Somit kann die Auswahl auf die wirklich Relevantet Dokumente für das entsprechende \gls{Keyword} vermindert werden.
            
\end{enumerate}







% ----------------------------------------------------------------

\section{Integration des Prototypen}
Basierend auf der ausgeführten Architektur (\autoref{architecture}) werden im folgenden Abschnitt die wichtigsten Software-Konzepte erläutert. Dazu werden als erstes die grundlegenden Algorithmen ausgeführt. In einem zweiten Schritt wird deren Integration in den Prototypen dargelegt.

Im Gegensatz zum Architektur-Entwurf befasst sich der Software-Entwurf neben den tatsächlich verwendeten Software-Lösungen auch mit der Recherche nach geeigneten Möglichkeiten.

\subsection{Übersicht}


Im Klassendiagramm auf \autoref{fig:prototypeClassDiagram-easy} sieht man einen Überblick über den zu entwickelnden Prototypen. Abgebildet sind die wichtigsten Klassen und deren Beziehungen untereinander. Zunächst liegen die beiden Teile \gls{ikc-core} und \texttt{Prototype} vor. Der \gls{ikc-core} ist der aus dem Forschungsprojekt \gls{IKC} herausgehende bestehende Prototyp. Dieser nimmt Gebrauch von den beiden vom \texttt{Prototypen} zur Verfügung gestellten Schnittstellen des \texttt{Index-} und des \texttt{DataServices}.

Grundsätzlich besteht der \texttt{Prototyp} aus den zwei Komponenten \texttt{Index-} und des \texttt{DataService}. Diese verwenden die von ausserhalb ver\-füg\-bar\-en Models, das \texttt{In\-dex-} und das \texttt{DataModel}. Diese enthalten die Protokolle für die jeweilige Kommunikation, werden somit auch vom \gls{ikc-core} in Anspruch genommen.

Im der Implementation \texttt{Index-} beziehungsweise \texttt{DataServices}.

    \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{PrototypeClassDiagram}
    \caption{Prototype Klassendiagram}
    \label{fig:prototypeClassDiagram-easy}
    \end{figure}

Der Kern der Software bildet der Algorithmus, dieser ist, neben der Suchfunktionalität und dem Aufbau der Indizes, ein Hauptbestandteil des \texttt{IndexService}. \autoref{fig:kommunikation} gewährt einen Überblick über die beteiligten Komponenten. Als Grundlage benötigt der \texttt{In\-dex\-Ser\-vice} alle zu indexierenden Dateien im Volltext. Deren Quelle ist der \texttt{Data\-Ser\-vice}. Der \texttt{Index-} und der \texttt{DataService} bilden zusammen den eigentlichen Prototypen. Wie in der Bausteinsicht auf \autoref{fig:bausteinsicht} zu erkennen, gibt es neben der Integration der Services auch eine Einbindung in die bestehende Benutzeroberfläche des \gls{ikc-core}.


\subsection{Schnittstellen}
Der Prototyp soll sich möglichst nahtlos in den \gls{ikc-core} integrieren. Um dies zu erreichen sollen in keiner Situation Funktionen des \gls{ikc-core} blockiert werden durch den Prototypen. 

So werden Suchresultate des Index innerhalb der bestehenden Suche integriert und bei Bedarf aktualisiert. Die verschieden Resultate der verschiedenen Quellen sollen in Echtzeit nach ihrem Eintreffen dargestellt werden. Somit wird sich die Liste mit Resultate trotz gleichem Suchbegriff über die Zeit verändern, da weitere Resultate von entfernten Quellen eintreffen. 

Weiter sollen extrahierte \gls{Keyword}[s] klar getrennt von den bestehenden Properties des Nodes als \textit{Chips} oberhalb des Titel dargestellt werden. Sowohl ein Dokument mit entsprechenden \gls{Keyword}[s] als auch eine \gls{Keyword} mit den verknüpften Dokumenten werden als Node dargestellt. \autoref{fig:bda_ui} zeigt einen Entwurf dieser Integration. 

    \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{BDA_UI}
    \caption{Entwurf Intgeration Benutzeroberfläche}
    \label{fig:bda_ui}
    \end{figure}

Um die Schnittstellen des Prototype ideal zu verwenden und die obigen Oberflächenanpassungen umzusetzten, sind Anpassungen bzw. Erweiterungen in der Software Struktur des \gls{ikc-core} nötig. Das Klassendiagramm (\autoref{fig:classDiagrammIkcCore-easy}) erläutert die wichstigsten Anpassungen:



    \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{ClassDiagrammIkcCore}
    \caption{Klassendiagramm Integration}
    \label{fig:classDiagrammIkcCore-easy}
    \end{figure}
%riesiges index.json, ungefähr 100k Files als Text-Dateien

\subsection{Datenfreigabe}
Für den Auftraggeber ist eine sichere Kommunikation und stetige Transparenz und Kontrolle über den Verbleib von benutzergenerierten Daten von hoher Wichtigkeit. Um diesen Anforderungen gerecht zu werden, wurde unter anderem ein Datenfreigabe-Konzept entwickelt. Dieses basiert auf \gls{Token}[s]. 






%ablaufdiagramm Einwegtoken Entkopplung

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{SeqAccessSession}
    \caption{Ablauf: Datenfreigabe}
    \label{fig:seqaccesssession-easy}
    \end{figure}
